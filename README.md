<div align="center">

<h1>Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction </h1>
</div>


![teaser](assets/teaser.png)
The first work aims to solve 3D Human Mesh Reconstruction task in **perspective-distorted images**. 



# ğŸ—“ï¸ News:
ğŸ† 2024.Jul.18, pretrained H48 model is released: https://huggingface.co/WenjiaWang/Zolly_ckpts

ğŸ† 2023.Nov.23, training code of Zolly is released, pretrained zolly weight will come soon.

ğŸ† 2023.Aug.12, Zolly is selected as ICCV2023 oral, [project page](https://wenjiawang0312.github.io/projects/zolly/).

ğŸ† 2023.Aug.7, the dataset link is released. The training code is coming soon.

ğŸ† 2023.Jul.14, Zolly is accepted to ICCV2023, codes and data will come soon.

ğŸ† 2023.Mar.27, [arxiv link](https://arxiv.org/abs/2303.13796) is released.


# ğŸš€ Run the code
## ğŸŒ Environments
- You should install [`MMHuman3D`](https://github.com/open-mmlab/mmhuman3d/blob/main/docs/install.md) firstly.

You should install the needed relies as `ffmpeg`, `torch`, `mmcv`, `pytorch3d` following its tutorials.


- It is recommended that you install the stable version of `MMHuman3D`:

```bash
wget https://github.com/open-mmlab/mmhuman3d/archive/refs/tags/v0.9.0.tar.gz;
tar -xvf v0.9.0.tar.gz;
cd mmhuman3d-0.9.0;
pip install -e .
```

You can install `pytorch3d` from file if you find any difficulty. 
E.g. `python3.8 + pytorch-1.13.1 + cuda-11.7 + pytorch3d-0.7.4`
```bash
wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch3d/linux-64/pytorch3d-0.7.4-py38_cu117_pyt1131.tar.bz2;
pip install fvcore;
pip install iopath;
conda install --use-local pytorch3d-0.7.4-py38_cu117_pyt1131.tar.bz2;
```

- install this repo
```bash
cd Zolly;
pip install -e .
```

## ğŸ“ Required Data and Files

You can download the files from [onedrive](https://connecthkuhk-my.sharepoint.com/:f:/g/personal/wwj2022_connect_hku_hk/EjwloRJZtVBBrgliQ76EP6YBMtVQ5G_D4TPo2n18CAwOyw).

This link contains:

- Dataset annotations:
 all have ground-truth focal length, translation and smpl parameters.
  - HuMMan (train, test_p1, test_p2, test_p3)
  - SPEC-MTP (test_p1, test_p2, test_p3)
  - PDHuman (train, test_p1, test_p2, test_p3, test_p4, test_p5)
  - 3DPW (train(has optimized neutral betas), test_p1, test_p2, test_p3)

- Dataset images.
  - HuMMan
  - SPEC-MTP
  - PDHuman
  - For other open sourced datasets, please downlad from their origin website.

- Pretrained backbone
  - `hrnetw48_coco_pose.pth`
  -  `resnet50_coco_pose.pth`

- Others
  - `smpl_uv_decomr.npz`
  - `mesh_downsampling.npz`
  - `J_regressor_h36m.npy`

- SMPL skinning weights
  - Please find in [SMPL official link](https://smpl.is.tue.mpg.de/).
  
## ğŸ‘‡ Arrange the files
<details>
<summary>Click here to unfold.</summary>

```bash
root
  â”œâ”€â”€ body_models
  â”‚Â Â  â””â”€â”€ smpl
  |       â”œâ”€â”€ J_regressor_extra.npy
  |       â”œâ”€â”€ J_regressor_h36m.npy
  |       â”œâ”€â”€ mesh_downsampling.npz
  |       â”œâ”€â”€ SMPL_FEMALE.pkl
  |       â”œâ”€â”€ SMPL_MALE.pkl
  |       â”œâ”€â”€ smpl_mean_params.npz
  |       â”œâ”€â”€ SMPL_NEUTRAL.pkl
  |       â””â”€â”€ smpl_uv_decomr.npz
  â”œâ”€â”€ cache
  â”œâ”€â”€ mmhuman_data
  â”‚Â Â  â”œâ”€â”€ datasets                                                                                
  |   â”‚   â”œâ”€â”€ coco                                                                                   
  |   â”‚   â”œâ”€â”€ h36m                                              
  |   â”‚   â”œâ”€â”€ humman                                            
  |   â”‚   â”œâ”€â”€ lspet                                             
  |   â”‚   â”œâ”€â”€ mpii                                              
  |   â”‚   â”œâ”€â”€ mpi_inf_3dhp                                      
  |   â”‚   â”œâ”€â”€ pdhuman                                                                                 
  |   â”‚   â”œâ”€â”€ pw3d                                              
  |   â”‚   â””â”€â”€ spec_mtp  
  â”‚Â Â  â””â”€â”€ preprocessed_datasets
  |       â”œâ”€â”€ humman_test_p1.npz
  |       â”œâ”€â”€ humman_train.npz
  |       â”œâ”€â”€ pdhuman_test_p1.npz
  |       â”œâ”€â”€ pdhuman_train.npz
  |       â”œâ”€â”€ pw3d_train.npz
  |       â”œâ”€â”€ pw3d_train_transl.npz
  |       â”œâ”€â”€ spec_mtp.npz
  |       â””â”€â”€ spec_mtp_p1.npz
  â””â”€â”€ pretrain
      â””â”€â”€ coco_pretrain 
        Â Â â”œâ”€â”€ hrnetw48_coco_pose.pth
        Â Â â””â”€â”€ resnet50_coco_pose.pth
```
</details>

 And change the `root` in `zolly/configs/base.py`

## ğŸš… Train
```bash
sh train_bash.sh zolly/configs/zolly_r50.py $num_gpu$ --work-dir=$your_workdir$
```
E.g, you can use
```bash
sh train_bash.sh zolly/configs/zolly_r50.py 8 --work-dir=work_dirs/zolly
```

To resume training or finetune model:
```bash
sh train_bash.sh zolly/configs/zolly_r50.py 8 --work-dir=work_dirs/zolly --resume-from work_dirs/zolly/latest.pth
```

## ğŸš— Test
```bash
sh test_bash.sh zolly/configs/zolly/zolly_r50.py $num_gpu$ --checkpoint=$your_ckpt$ --data-name pw3d
```
For convenience, you can test the first 100 samples to evaluate your model.
```bash
sh test_bash.sh zolly/configs/zolly/zolly_r50.py $num_gpu$ --checkpoint=$your_ckpt$ --data-name pw3d --num-data 100
```

## ğŸ® Demo images in a folder
```bash
sh demo_bash.sh zolly/configs/zolly/zolly_h48.py $num_gpu$ --checkpoint=$your_ckpt$ --image_folder assets/demo_jpg --ext jpg --demo_root demo/
```
The output name will be like `56_789-0.00_586-1.91_pred.png`, which represent `{raw_name}_{gt_f}-{gt_z}_{pred_f}-{pred_z}_pred.png`

<img src="assets/demo2.jpg" alt="Description" style="width: 35%;"> <img src="assets/demo.jpg" alt="Description" style="width: 38%;">


## Pretrained Models:

We have released our H48 model on huggingface:
https://huggingface.co/WenjiaWang/Zolly_ckpts

You can use `huggingface-cli download WenjiaWang/Zolly_ckpts --repo-type model` to download the model. (Remember to login with you token firstly)

R50 is not ready yet, please wait.


- We re-trained our method and update the results for 3DPW:

|  Method | PA-MPJPE| MPJPE | PA-PVE | PVE |
|----------|----------|----------|----------|----------|
| Zolly-H48 | 47.88 | 78.21 | 63.55  |  90.82  |
| Zolly-H48(ft) | 39.09 | 64.44 | 51.49  |  75.78  |


## ğŸ’»Add Your Own Algorithm
- Add your own network in `zolly/models/heads`, and add it to `zolly/models/builder.py`.
- Add your own trainer in `zolly/models/architectures`, and add it to `zolly/models/architectures/builder.py`.
- Add your own loss function in `zolly/models/losses`, and add it to `zolly/models/losses/builder.py`.
- Add your own config file in `zolly/configs/`, you can modify from `zolly/configs/zolly_r50.py`. And remember to change the `root` parameter in `zolly/configs/base.py`, where your files should be put.

# ğŸ“ Citation

If you find this project useful in your research, please consider cite:

```
@inproceedings{wangzolly,
  title={Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction Supplementary Material},
  author={Wang, Wenjia and Ge, Yongtao and Mei, Haiyi and Cai, Zhongang and Sun, Qingping and Wang, Yanjun and Shen, Chunhua and Yang, Lei and Komura, Taku},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
  year={2023}
}
```
# ğŸ˜ Acknowledge
Emojis are collected from [gist:7360908](https://gist.github.com/rxaviers/7360908#gistcomment-4745876).

Some of the codes are based on [`MMHuman3D`](https://github.com/open-mmlab/mmhuman3d/blob/main/docs/install.md), [`DecoMR`](https://github.com/zengwang430521/DecoMR).

# ğŸ“§ Contact

Feel free to contact me for other questions or cooperation: wwj2022@connect.hku.hk
